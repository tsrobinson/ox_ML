for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
cces$votetrump <- as.factor(cces$votetrump)
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
#### 2. Get an intuition for the stacking procedure ####
## NB. we won't cross-validate our results here or create a test dataset
## Estimate some models
# LASSO
# Generate dummy variables (because glmnet doesn't like factors)
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=x_train[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(x_train$birthyr, fac_mod_mat)
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 4/5                                 ##
##                                                                            ##
##                              Ensemble Methods                              ##
##                                  Walkthrough                               ##
##                                                                            ##
################################################################################
# This walkthrough uses the Cooperative Congressional Election Survey 2018 data
# Surveys US voters across all US states + D.C.
# Full data and codebook are available at https://doi.org/10.7910/DVN/ZSBZ7K
#### 0. System setup ####
## Set up commands -- follow this to set up on your own machine
# install.packages("SuperLearner")
# install.packages(c("caret", "glmnet", "randomForest", "ggplot2", "RhpcBLASctl"))
#### 1. Load packages and data ####
library(SuperLearner)
library(glmnet)
library(randomForest)
set.seed(89)
cces <- read.csv("data/cces_formatted_oxml.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
cces$votetrump <- as.factor(cces$votetrump)
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
y_train
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)
class(y_train)
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
# Generate dummy variables (because glmnet doesn't like factors)
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=x_train[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(x_train$birthyr, fac_mod_mat)
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)$lambda.min
lasso_mod <- glmnet(x = mod_mat, y = y_train, alpha = 1, lambda = cv_lambda)
lasso_mod
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 4/5                                 ##
##                                                                            ##
##                              Ensemble Methods                              ##
##                                  Walkthrough                               ##
##                                                                            ##
################################################################################
# This walkthrough uses the Cooperative Congressional Election Survey 2018 data
# Surveys US voters across all US states + D.C.
# Full data and codebook are available at https://doi.org/10.7910/DVN/ZSBZ7K
#### 0. System setup ####
## Set up commands -- follow this to set up on your own machine
# install.packages("SuperLearner")
# install.packages(c("caret", "glmnet", "randomForest", "ggplot2", "RhpcBLASctl"))
#### 1. Load packages and data ####
library(SuperLearner)
library(glmnet)
library(randomForest)
set.seed(89)
cces <- read.csv("data/cces_formatted_oxml.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
View(lasso_mod)
predict(lass_mod)
predict(lasso_mod)
predict(lasso_mod, newx = x_train)
predict(lasso_mod, newx = mod_mat)
y_train
name(y_train)
rf_model <- randomForest(votetrump ~ .,
data = cbind(votetrump = y_train, x_train))
rf_model <- randomForest(votetrump ~ .,
data = cbind(votetrump = as.factor(y_train), x_train))
predict.randomForest
?predict.randomForest
rf_yhat_train <- predict(rf_model, type = "prob")
rf_yhat_train
predict(rf_model, type = "prob")[,2]
predict(rf_model, type = "prob")
rf_yhat_train <- predict(rf_model, type = "prob")[,2]
rf_yhat_train
as.formula("~","a+b")
as.formula(~,"a+b")
paste0("votetrump ~ ",paste0(train_vars, collapse = " + "))
full_train <- cbind(votetrump = y_train, x_train)
logit_model <- glm(paste0("votetrump ~ ",paste0(train_vars, collapse = " + ")),
data = full_train)
logit_yhat_train <- predict(logit_model)
logit_model
logit_yhat_train
?predict.glm
logit_model <- glm(paste0("votetrump ~ ",paste0(train_vars, collapse = " + ")),
data = cbind(votetrump = y_train, x_train))
logit_yhat_train <- predict(logit_model)
# Generate dummy variables (because glmnet doesn't like factors)
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=x_train[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(x_train$birthyr, fac_mod_mat)
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)$lambda.min
lasso_mod <- glmnet(x = mod_mat, y = y_train, alpha = 1, lambda = cv_lambda)
lasso_yhat_train <- predict(lasso_mod, newx = mod_mat)
rf_model <- randomForest(votetrump ~ .,
data = cbind(votetrump = as.factor(y_train), x_train))
rf_yhat_train <- predict(rf_model, type = "prob")[,2]
lasso_yhat_train
View(lasso_yhat_train)
as.numeric(lasso_yhat_train)
mod_preds <- data.frame(logit = as.numeric(logit_yhat_train),
lasso = as.numeric(lasso_yhat_train),
rf = as.numeric(rf_yhat_train))
View(mod_preds)
library(nlmrt)
numeric(3)
list(b1 = 1, b2 = 2, b3 = 3)
?nlxb
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 1, b2 = 2, b3 = 3))
mod_preds <- data.frame(y = y_train,
logit = as.numeric(logit_yhat_train),
lasso = as.numeric(lasso_yhat_train),
rf = as.numeric(rf_yhat_train))
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 1, b2 = 2, b3 = 3))
mod_preds <- data.frame(Y = y_train,
logit = as.numeric(logit_yhat_train),
lasso = as.numeric(lasso_yhat_train),
rf = as.numeric(rf_yhat_train))
library(nlmrt)
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 1, b2 = 2, b3 = 3))
summary(stack_model)
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 1, b2 = 2, b3 = 3))
View(stack_model)
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 0, b2 = 0, b3 = 0))
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + b3 * rf)/(b1 + b2 + b3),
data = mod_preds,
lower = numeric(3),
start = list(b1 = 1/3, b2 = 1/3, b3 = 1/3))
stack_model$coefficients
sum(stack_model$coefficients)
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + (1-b1-b2) * rf),
data = mod_preds,
lower = numeric(2),
start = list(b1 = 1/3, b2 = 1/3))
stack_model$coefficients
library(nlmrt)
stack_model$coefficients
# Generate dummy variables (because glmnet doesn't like factors)
lasso_format <- function(X) {
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=X[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(X$birthyr, fac_mod_mat)
}
x_train_lasso <- lasso_format(x_train)
# Generate dummy variables (because glmnet doesn't like factors)
lasso_format <- function(X) {
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=X[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(X$birthyr, fac_mod_mat)
return(mod_mat)
}
x_train_lasso <- lasso_format(x_train)
cv_lambda <- cv.glmnet(x = mod_mat, y = y_train, alpha = 1)$lambda.min
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 4/5                                 ##
##                                                                            ##
##                              Ensemble Methods                              ##
##                                  Walkthrough                               ##
##                                                                            ##
################################################################################
# This walkthrough uses the Cooperative Congressional Election Survey 2018 data
# Surveys US voters across all US states + D.C.
# Full data and codebook are available at https://doi.org/10.7910/DVN/ZSBZ7K
#### 0. System setup ####
## Set up commands -- follow this to set up on your own machine
# install.packages("glmnet")
# install.packages("randomForest")
# install.packages("nlmrt")
#### 1. Load packages and data ####
library(glmnet)
library(randomForest)
library(nlmrt)
set.seed(89)
cces <- read.csv("data/cces_formatted_oxml.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
#### 2. Get an intuition for the stacking procedure ####
## NB. we won't cross-validate our results here or create a test dataset
## Estimate some models
# Logistic
logit_model <- glm(paste0("votetrump ~ ",paste0(train_vars, collapse = " + ")),
data = cbind(votetrump = y_train, x_train))
logit_yhat_train <- predict(logit_model)
# LASSO
# Generate dummy variables (because glmnet doesn't like factors)
lasso_format <- function(X) {
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=X[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(X$birthyr, fac_mod_mat)
return(mod_mat)
}
x_train_lasso <- lasso_format(x_train)
cv_lambda <- cv.glmnet(x = x_train_lasso, y = y_train, alpha = 1)$lambda.min
lasso_mod <- glmnet(x = x_train_lasso, y = y_train, alpha = 1, lambda = cv_lambda)
lasso_yhat_train <- predict(lasso_mod, newx = x_train_lasso)
# Inspect the coefficients
stack_model$coefficients
predict
?predict
predict.glm
?predict.glm
?predict.randomForest
y_logit <- predict(logit, newdata = x_test)
y_logit <- predict(logit_model, newdata = x_test)
y_logit <- predict(logit_model, newdata = x_test)
y_lasso <-  predict(lasso_mod, newx = lasso_format(x_test))
y_rf <- predict(rf_model, newdata = x_test, type = "prob")[,2]
?matrix
y_logit <- predict(logit_model, newdata = x_test)
y_lasso <-  predict(lasso_mod, newx = lasso_format(x_test))
y_rf <- predict(rf_model, newdata = x_test, type = "prob")[,2]
stacked_pred <- matrix(as.numeric(y_logit),
as.numeric(y_lasso),
as.numeric(y_rf))
stacked_pred <- matrix(nrow = nrow(x_test), ncol = 3)
stacked_pred <- matrix(nrow = nrow(x_test), ncol = 3)
stacked_pred[,1] <- as.numeric(y_logit)
stacked_pred[,2] <- as.numeric(y_lasso)
stacked_pred[,3] <- as.numeric(y_rf)
View(stacked_pred)
# Inspect the coefficients
wgts <- c(stack_model$coefficients, 1 - sum(stack_model$coefficients))
stacked_pred %*% wgts
yhat_test <- stacked_pred %*% wgts
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 5/5                                 ##
##                                                                            ##
##                                Ensemble Methods                            ##
##                                  Walkthrough                               ##
##                                                                            ##
################################################################################
# This walkthrough uses the Cooperative Congressional Election Survey 2018 data
# Surveys US voters across all US states + D.C.
# Full data and codebook are available at https://doi.org/10.7910/DVN/ZSBZ7K
#### 0. System setup ####
## Set up commands -- follow this to set up on your own machine
# install.packages("glmnet")
# install.packages("randomForest")
# install.packages("nlmrt")
#### 1. Load packages and data ####
library(glmnet)
library(randomForest)
library(nlmrt)
set.seed(89)
cces <- read.csv("data/cces_formatted_oxml.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
# Recode outcome variable
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
# Get a 2/3, 1/3 split of train and test data
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
logit_model <- glm(paste0("votetrump ~ ",paste0(train_vars, collapse = " + ")),
data = cbind(votetrump = y_train, x_train))
# Generate dummy variables (because glmnet doesn't like factors)
lasso_format <- function(X) {
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=X[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(X$birthyr, fac_mod_mat)
return(mod_mat)
}
x_train_lasso <- lasso_format(x_train)
cv_lambda <- cv.glmnet(x = x_train_lasso, y = y_train, alpha = 1)$lambda.min
lasso_mod <- glmnet(x = x_train_lasso, y = y_train, alpha = 1, lambda = cv_lambda)
rf_model <- randomForest(votetrump ~ .,
data = cbind(votetrump = as.factor(y_train), x_train))
# Get predictions on training data
logit_yhat_train <- predict(logit_model)
lasso_yhat_train <- predict(lasso_mod, newx = x_train_lasso)
rf_yhat_train <- predict(rf_model, type = "prob")[,2]
train_preds <- data.frame(Y = y_train,
logit = as.numeric(logit_yhat_train),
lasso = as.numeric(lasso_yhat_train),
rf = as.numeric(rf_yhat_train))
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + (1-b1-b2) * rf),
data = train_preds,
lower = numeric(2),
start = list(b1 = 1/3, b2 = 1/3))
# Inspect the coefficients
wgts <- c(stack_model$coefficients, 1 - sum(stack_model$coefficients))
y_logit <- predict(logit_model, newdata = x_test)
y_lasso <-  predict(lasso_mod, newx = lasso_format(x_test))
y_rf <- predict(rf_model, newdata = x_test, type = "prob")[,2]
stacked_pred <- cbind(as.numeric(y_logit),
as.numeric(y_lasso),
as.numeric(y_rf))
yhat_test <- stacked_pred %*% wgts
mean(yhat_test == y_test)
View(yhat_test)
mean(round(yhat_test) == y_test)
mean(round(yhat_test) == y_test)
y_rf
round(y_rf)
mean(round(y_rf) == y_test)
mean(round(yhat_test) == y_test)
mean((yhat_test - y_test)^2)
mean((y_rf - y_test)^2)
mean((y_lasso - y_test)^2)
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 5/5                                 ##
##                                                                            ##
##                                Ensemble Methods                            ##
##                                  Walkthrough                               ##
##                                                                            ##
################################################################################
# This walkthrough uses the Cooperative Congressional Election Survey 2018 data
# Surveys US voters across all US states + D.C.
# Full data and codebook are available at https://doi.org/10.7910/DVN/ZSBZ7K
#### 0. System setup ####
## Set up commands -- follow this to set up on your own machine
# install.packages("glmnet")
# install.packages("randomForest")
# install.packages("nlmrt")
#### 1. Load packages and data ####
library(glmnet)
library(randomForest)
library(nlmrt)
set.seed(89)
cces <- read.csv("data/cces_formatted_oxml.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
# Recode outcome variable
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", 1, 0)
# Get an approximate 2:1 split of train and test data
train_indices <- sample(1:nrow(cces), 30000)
test_indices <-  setdiff(1:nrow(cces), train_indices)
train_vars <- c("birthyr", "gender", "sexuality", "trans", "educ", "votereg", "race")
x_train <- cces[train_indices, train_vars]
x_test <- cces[test_indices, train_vars]
y_train <- cces$votetrump[train_indices]
y_test <- cces$votetrump[test_indices]
#### 2. Estimate individual models ####
## NB. we won't cross-validate our results here (except choosing LASSO lambda)
## Estimate some models
# Logistic
logit_model <- glm(paste0("votetrump ~ ",paste0(train_vars, collapse = " + ")),
data = cbind(votetrump = y_train, x_train))
# LASSO
# Generate dummy variables (because glmnet doesn't like factors)
lasso_format <- function(X) {
cat_vars <- c("gender","sexuality","trans","educ","votereg","race")
contr.list <- lapply(1:length(cat_vars), function (x) contr.sum)
names(contr.list) <- paste0("factor(",cat_vars,")")
fac_mod_mat <- model.matrix(as.formula(paste0("~", paste0(names(contr.list), collapse = " + "))),
data=X[,cat_vars],contrasts.arg=contr.list)[,-1]
mod_mat <- cbind(X$birthyr, fac_mod_mat)
return(mod_mat)
}
x_train_lasso <- lasso_format(x_train)
cv_lambda <- cv.glmnet(x = x_train_lasso, y = y_train, alpha = 1)$lambda.min
lasso_mod <- glmnet(x = x_train_lasso, y = y_train, alpha = 1, lambda = cv_lambda)
## Random Forest
rf_model <- randomForest(votetrump ~ .,
data = cbind(votetrump = as.factor(y_train), x_train))
#### 3. Generate stacking model ####
# Get predictions on training data
logit_yhat_train <- predict(logit_model)
lasso_yhat_train <- predict(lasso_mod, newx = x_train_lasso)
rf_yhat_train <- predict(rf_model, type = "prob")[,2]
train_preds <- data.frame(Y = y_train,
logit = as.numeric(logit_yhat_train),
lasso = as.numeric(lasso_yhat_train),
rf = as.numeric(rf_yhat_train))
## Estimate the weights using nonlinear least squares estimator
stack_model <- nlxb(Y ~ (b1 * logit + b2 * lasso + (1-b1-b2) * rf),
data = train_preds,
lower = numeric(2),
start = list(b1 = 1/3, b2 = 1/3))
# Inspect the coefficients
wgts <- c(stack_model$coefficients, 1 - sum(stack_model$coefficients))
#### 4. Generate a stacked predictor function
y_logit <- predict(logit_model, newdata = x_test)
y_lasso <-  predict(lasso_mod, newx = lasso_format(x_test))
y_rf <- predict(rf_model, newdata = x_test, type = "prob")[,2]
stacked_pred <- cbind(as.numeric(y_logit),
as.numeric(y_lasso),
as.numeric(y_rf))
yhat_test <- stacked_pred %*% wgts
# Accuracy
mean(round(yhat_test) == y_test)
#### Extension exercise ####
## 1. Using the various scripts we have gone through this week, can you compare
# the mean squared error of the logit, LASSO, and RF probabilities to the mean
# squared error of the stacked estimator?
mean((yhat_test - y_test)^2)
mean((y_rf - y_test)^2)
mean((y_logit - y_test)^2)
mean((y_lasso - y_test)^2)
