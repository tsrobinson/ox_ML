---
title: "Neural Networks (and deep learning)"
subtitle: "Oxford Spring School in Advanced Research Methods, 2021"
author: "Dr Thomas Robinson, Durham University"
date: "Day 4/5"
output: beamer_presentation
header-includes:
  - \usepackage{wrapfig}
  - \usepackage{graphicx}
  - \usepackage{bm}
  - \input{ml_sty.tex}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```

## Recap

Yesterday's session, we covered non-parametric ML methods using trees

* The estimator does not require the model form to be pre-specified

* Complex relationships can be captured via a conjunction of splits

* Two different "forest" methods can improve on the performance of CART

## Today's session

Explore an even more flexible form of ML:

* Neural network architectures
* An "engineering-grade" solution
  * A lot more flexible in terms of abstract structure than forests
* Introduce the concept of deep-learning

Remainder of the session:

1. Simple neural networks
2. Deep learning
3. Different types of neural network
4. Example application in social science

# Neural network terminology

## Basic terminology

**Input**

* Data that forms an argument passed to the node

**Weight**

* A scalar that is *multiplied* with an input vector

**Bias**

* A scalar value *added* to the input vector

**Node**

* A computational unit that applies some function $f$ to inputs, weights and bias

**Output**

* The result of the function applied at a node -- typically another vector

## Simple perceptron model

Suppose we have a single vector of input data $\bm{x}$, and we want to predict the output $\bm{y}$

A simple perceptron model looks like the following:

![Single node, single layer perceptron model](images/sn_perceptron.png){width=40%}

$w$ is the weight term and $b$ is the bias term -- in this simple case, both are scalar.


## Activation functions $\phi$

The activation function is simply a function applied to the result of $w\bm{x} + b$, that controls the range of the output vector

$\phi$ may simply be the **identity function**:

* I.e. $\phi(\bm{x}) = \bm{x}$

**Sigmoid function**:

* $\phi(\bm{x}) = \frac{1}{1 + e^{-\bm{x}}}$

**Rectified Linear Unit (ReLU)**:

* $\phi(\bm{x}) = \max(0,x)$

**Tanh**:

* $\phi(\bm{x}) = \frac{e^{\bm{x}} - e^{-\bm{x}}}{e^{\bm{x}} + e^{-\bm{x}}}$

These functions (and others) are particularly useful because they have known derivatives -- which we'll return to later!

## Gaining a prediction from our simple model

\begin{columns}
\begin{column}{0.4\textwidth}
\includegraphics[width = \textwidth]{images/sn_perceptron.png}
\end{column}
\begin{column}{0.48\textwidth}
Suppose:
\begin{itemize}
\item $\phi$ is the ReLU function
\item $\bm{w} = \bm{2}, b = 1$
\end{itemize}
\end{column}
\end{columns}

And we observe the following input vector $\bm{x}$:
$$
\begin{bmatrix}
5 \\
1 \\
-1 
\end{bmatrix}  
$$
What is $\hat{\bm{y}}$?

## Multiple inputs

The first model is very basic, so we can adapt it to accept **multiple** inputs:

* Let $k$ index input variables, i.e. $\bm{X} = \{\bm{x}_1,\ldots,\bm{x}_k\}$
* Let $\bm{w}$ be a vector of weights, i.e. $\bm{w} = \{w_1, \ldots, w_k \}$

Inside our activation function we replace $w\bm{x} + b$ with
$$
w_1\bm{x}_1 + \ldots + w_k\bm{x}_k + b \equiv \sum_k{w_k\bm{x}_k} + b
$$

![Single node, multiple input perceptron model](images/mn_perceptron.png){width=40%}

## Initialisation

Like BART (and unlike CART/RF) prior to training we build the model network

For a single-node perceptron model with $k$ inputs, that means instantiating the weights and biases

* A naive option sets $\bm{w} = \bm{0}$

  * This is rarely optimal -- it can lead to significantly slower convergence (and can even disrupt convergence entirely)
  
A now standard approach is to use **Xavier initialisation** where:
$$
w_k \sim \mathcal{N}(0,\frac{1}{k})
$$

  * where $k$ is the number of inputs to the node
  * Typically used when $\phi$ is tanh or sigmoidal
  * Bias terms are instantiated at zero


## Loss functions

The goal of the perceptron is to minimise the predictive error between $\bm{y}$ and $\bm{\hat{y}} = \phi(\sum_k{w_k\bm{x}_k} + b)$

Depending on the type of prediction problem, we want to use a different function:

**Continuous $\bm{y}$**

* $\phi$ will be linear or ReLU 
* Minimise the mean squared error
* I.e. $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$

**Binary $\bm{y}$**

* $\phi$ will be sigmoid
* Minimise using **cross-entropy** loss function
* I.e. $=-\frac{1}{N}\sum_{i=1}^n\sum_{c=1}^{C}{y_{ic}\log(\hat{y_{ic}})}$ 
    * where $c$ indexes the classes within the binary/categorical variable

## OLS/Logistic regression as a single-layer perceptron

We can construe OLS as a single-node perceptron model,
$$
\bm{y} = \phi(b + w_1\bm{x_1} + ... + w_k\bm{x_k}),
$$
when:

* $\phi$ is the identity function
* $\bm{w}$ is the regression coefficient vector
* $b$ is the intercept

and solved via MLE.

Similarly logistic regression is where $\phi$ is the sigmoid activation function.

## Limitations and extensions

A single-node perceptron model is not particularly exciting:

* With identity/sigmoid activation functions we get conventional estimators
* The model is linear in inputs

To complicate our models we need to think about creating a **network** of nodes

* Increase the number of computational units
* Determine the flow of information along the network

Similar to how tree-based methods add complexity by branching

* Multiple nodes, and interactions between nodes, allow us to model complicated relationships


# Deep learning

## Complicating the network

![Multi-layer (but not deep) network](images/multi-layer.png){width=80%}

## Deep neural network

![Multi-layer **deep** network](images/deep-network.png){width=80%}

## Multi-layer network notation

The computation of outputs through layer $h$ of a neural network is:
\begin{align*}
\mathbf{y}^{(h)} = \sigma ( \mathbf{W}^{(h)} \mathbf{y}^{(h-1)} + \mathbf{b}^{(h)} ), \vspace{-1em}
\end{align*}
where:

* $\mathbf{y}^{(h)}$ is a vector of outputs from layer $h$
* $\mathbf{W}^{(h)}$ is a matrix of weights for layer $h$
* $\mathbf{b}$ is a vector of biases for layer $h$
* $\sigma$ is an activation function

This model can be generalized to an arbitrary number of hidden layers $H$:
\begin{align*}@
\mathbf{y} = \Phi ( \mathbf{W}^{(H)}[...[\sigma ( \mathbf{W}^{(2)}  [\sigma (\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})] + \mathbf{b}^{(2)})]...] + \mathbf{b}^{(H)}),
\end{align*}
where $\mathbf{x}$ is a vector of inputs and $\Phi$ is a final-layer activation function.

## Fully-connected networks

In a fully connected network:

* Every output from layer $h$ is an input to every node in layer $h+1$

![Fully-connected neural network](images/fully-connected.png){width=80%}

## Feed-forward training

We initialise a multi-layer model like a single-layer model:

1. Set weight terms for each node within each layer via some initialisation function

During training, an **epoch** consists of:

2. Feeding every observation through the model
  * When there are no cycles in the network, this is called "feed-forward"

3. Calculate the loss associated with the prediction

4. Adjust weights and biases based on the gradient of the loss

  * This is complicated with multiple layers
  * Adjusting the weights and bias affects the output of a node.
  * ... and the input of the nodes (plural!) that it feeds into!

*Repeat steps 2-4 multiple times*


## Backpropagation

We calculate the loss as a function of the known outcomes ($\bm{y}$) and predicted output ($\hat{\bm{y}}$), plus a weight decay term to prevent overfitting:

$$
E = L(\mathbf{y}, \hat{\mathbf{y}}) + \lambda ||\mathbb{E}[\mathbf{W}]||_2
$$

To estimate gradient of the loss function, we can calculate it sequentially from the final layer to the first.

\vspace{1em}

Once the gradient is calculated, we adjust the weights by some scaled amount $\gamma$ (the learning rate):
$$
\Delta \mathbf{W}^{(h)} = - \gamma \frac{\partial E}{\partial \mathbf{W}^{(h)}}
$$

## Optimisation details

Through two applications of the chain rule, the change in loss due to the weights in layer $h$ is,
\begin{align}
    \frac{\partial E}{\partial \mathbf{W}^{(h)}} &= \frac{\partial E}{\partial \mathbf{y}^{(h)}} \cdot \frac{\partial \mathbf{y}^{(h)}}{\partial \mathbf{W}^{(h)}} \\
    &= \frac{\partial E}{\partial \mathbf{y}^{(h+1)}} \cdot \frac{\partial \mathbf{y}^{(h+1)}}{\partial \mathbf{y}^{(h)}} \cdot \frac{\partial \mathbf{y}^{(h)}}{\partial \mathbf{W}^{(h)}}.
\end{align}

Estimating the terms:

* $\frac{\partial E}{\partial \mathbf{y}^{(h+1)}}$ -- the derivative w.r.t. the outputs from the next layer
* $\frac{\partial \mathbf{y}^{(h+1)}}{\partial \mathbf{y}^{(h)}}$ -- the derivative of the next layer's activation function
* $\frac{\partial \mathbf{y}^{(h)}}{\partial \mathbf{W}^{(h)}}$ -- equal to $\mathbf{y}^{(h-1)}$, since $\mathbf{y}^{(h)}$ is the weighted sum of the inputs into layer $h$

## Recap of neural networks

![Developing more complex predictive networks](images/neural_nets.pdf)

## Advantages and Limitations

Deep neural networks are extremely powerful

* Currently in use across industry to solve prediction problems
* Help make film recommendations, target ads, predict complex physical processes 

Neural networks rest on the aggregated power of many simple models

* Each node is a linear combination of inputs
* Each activation function will have known derivatives etc.


But deep neural networks are **black boxes**

* The hidden layers are near-unintelligible in substantive terms
* We still do not fully understand their performance!


# Types of Neural Network

## "Engineering-grade" solution: design

Neural networks have huge design degrees of freedom 

* Number of nodes
* Number of layers
* Activation functions
* Connections between nodes
  * Feed-forward or recurrent?
  * Fully-connected?
* The number of networks (!)
* The number of training epochs

We can construct different structures to help us solve different types of problem

* We'll explore a few types in remainder of this session

## "Engineering-grade" solution: computation

Neural network technology is rapidly developing

* We can parallelise operations over CPUs and GPUs
* New types of algebraic objects like tensors that make computation efficient
  * And corresponding new technology like tensor processing units (TPUs)
  
![Google Tensor Processing Unit rack](images/tensor.png)
  

## Autoencoders

Classical autoencoders consist of two parts.

\vspace{1em}

An \textit{encoder} deterministically maps an input vector $\mathbf{x}$ to a lower-dimensional representation $\mathbf{y}$:
\small
\begin{align*}
\mathbf{y} = f_\theta(\mathbf{x}) = \sigma( \mathbf{W}^{(B)}[...[\sigma ( \mathbf{W}^{(2)}  [\sigma (\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})] + \mathbf{b}^{(2)})]...] + \mathbf{b}^{(B)}),
\end{align*}
\normalsize
where $B$ is the "bottleneck" layer consisting of fewer nodes than the input layer.

\vspace{1em}
A \textit{decoder} then maps $\mathbf{y}$ back to a reconstructed vector $\mathbf{z}$ of the same dimensions as $\mathbf{x}$:
\footnotesize
\begin{align*}
\mathbf{z} = g_{\theta^\prime} (\mathbf{y}) = \Phi ( \mathbf{W}^{(H) \prime}[...[\sigma ( \mathbf{W}^{(B+2) \prime}  [\sigma (\mathbf{W}^{(B+1) \prime} \mathbf{y} + \mathbf{b}^{(B+1) \prime})] + \mathbf{b}^{(B+2) \prime})]...] + \mathbf{b}^{(H) \prime})
\end{align*}

## Autoencoder graph

![Graphical depiction of an autoencoder network](images/autoencoder.png)

## Uses and limitations of classical autoencoders

Autoencoders perform dimensionality reduction:

* Find a lower-dimensional representation $\mathbf{y}$ that captures the "essence" of $\mathbf{x}$
* Could be repurposed for finding latent concepts in social science
  * Where decoding element is not necessary

It is quite easy to learn/approximate the **identity function**:

* Combination of nodes perfectly reconstructs input through layers of the network
* Leads to loss values of zero in-training
* And likely very poor performance out-of-sample
* I.e. overfitting

*When we face a problem of overfitting what do we do?*

## Denoising autoencoders as regularised autoencoders

Denoising autoencoders corrupt the input:
$$\mathbf{x} \rightarrow \tilde{\mathbf{x}} \sim q_D (\mathbf{x}|\tilde{\mathbf{x}}).$$ 

The corrupted input is then mapped to a hidden representation 
$$\mathbf{y} = f_{\theta}(\tilde{\mathbf{x}}),$$ 
from which a clean or ``denoised'' version is reconstructed:
$$\mathbf{z} = g_{\theta^\prime}(\mathbf{y}).$$

Unlike before, $\mathbf{\hat{y}}$ is now a deterministic function of $\tilde{\mathbf{x}}$ (not $\mathbf{x}$).

* Loss is calculated with respect to the original input data $\bm{x}$ and "cleaned" data
* Hence learning identity function is not optimal in training

## Generative Adversarial Networks

Given some training data, how might we produce new data that have the same features:

* Given a series of text, can we produce new "sensible" text?
* Given a set of images, can we create new images that would plausibly belong to that set?

Generative adversarial networks split this problem in two:

* **Model A** is trained to *generate* new content based on $\bm{X}$
  * Denote this $\bm{X'}^{\text{Gen.}}$
  * Let $\bm{X'}$ be the set $\{\bm{X'}^{\text{Real}},\bm{X'}^{\text{Gen.}}\}$

* **Model B** is trained to detect whether some $\bm{x'} \in \bm{X'}$ is "Gen." or "Real"

## GAN optimisation

We have two networks with **competing** (hence adversarial) goals:

* **Model A** wants to maximise the error of **Model B**
* **Model B** wants to minimise its prediction error

When trained in tandem, **Model A** tries to create ever more realistic new observations that **Model B** cannot distinguish

* In turn, **Model B** gets more discriminant over time
* Forcing **Model A** to improve!

GAN-generated portraits:

![NVidia StyleGAN2 synthetic images](images/gans.png)

# Imputing missing data using deep learning

## The problem of missing data

We often use data that has missing values:

* E.g. Respondents may refuse to answer a question, or trace data is corrupted or imperfectly recorded
* Leads to imprecision when data is missing \textit{completely} at random (MCAR)
* Or bias when data is missing at random (MAR) or missing not at random (MNAR)

One solution is to use multiple imputation (MI) to ``complete'' the datasets:

* Predict missing values \textit{multiple} times to account for uncertainty
* Estimate separate models then combine the results

## Combination rules

Once we have $M$ completed datasets, we:

* Estimate $M$ separate regression models
* Combine the coefficients using Rubin's (1987) combination rules:

$$\hat{\beta} = \frac{1}{M} \left( \sum_{i=1}^M\hat{\beta}_{i} \right)$$

$$\widehat{\sigma^2} = \widehat{\sigma^2}_W + \left(1+\frac{1}{M}\right)\widehat{\sigma^2}_B,$$

where:
$$\widehat{\sigma^2}_W = \frac{1}{M} \left( \sum_{i=1}^{M} \widehat{\sigma^2}_i \right)$$
$$\widehat{\sigma^2}_B = \sqrt{\frac{\sum_{i=1}^{M}(\hat{\beta}_i - \hat{\beta})^2}{M-1}}$$


## Problem of existing MI strategies

Existing MI strategies struggle to handle large and complex data

* Poor at capturing non-parametric or non-linear relationships
    * Restrictive assumptions about joint distributions
    * Weak or erroneous predictions of missing datapoints
* Face significant computational bottlenecks

Categorical problems:

  * Suppose we have a variable with 5 classes
    * To handle statistically, we **one-hot encode** predictor into 5 binary columns
  * If we have a dataset with 3 continuous predictors and 3 5-class categorical variables
    * Nominal "width" of the data is 6 columns
    * Effective "width" is 18 columns
  * If you account for the dependence across columns, categorical variables become computationally taxing

## MIDAS neural network (Lall and Robinson 2021)

![MIDAS network structure](images/midas_network.pdf)

## Dropout

Most of today's session focused on fully-connected graphs:

* Easy to implement
* Yield a highly flexible prediction apparatus

But less good for inferential purposes

* High bias method
* Sometimes we want to add some variance back in
* For example, when the goal is to generate *multiple* predictions

Dropout is a regularisation process where we randomly "drop" nodes

* This occurs randomly for each epoch and draw
* Goal is to force the network to model a distribution of functions that describe the data
  * Approximating a **Gaussian Process**

## MIDAS network with dropout

Dropout involves multiplying outputs from each layer by a Bernoulli vector $\mathbf{v}$:
$$\tilde{\mathbf{y}}^{(h)} = \mathbf{v}^{(h)} \mathbf{y}^{(h)}, \mathbf{v}^{(h)} \sim \text{Bernoulli}(p).$$

* When $v^{(h)}_i = 0$ then that output adds nothing to any connected nodes in the next layer

The trained denoising encoder with dropout can thus be described as:
\footnotesize
\begin{align*}
\tilde{\mathbf{y}} = f_\theta(\tilde{\mathbf{x}}) = \sigma( \mathbf{W}^{(B)} \mathbf{v}^{(B)} [...[\sigma ( \mathbf{W}^{(2)} \mathbf{v}^{(2)}  [\sigma (\mathbf{W}^{(1)} \tilde{\mathbf{x}} + \mathbf{b}^{(1)})] + \mathbf{b}^{(2)})]...] + \mathbf{b}^{(B)}).
\end{align*}

\normalsize
The decoder, in turn, becomes:
\footnotesize
\begin{align*}
\mathbf{z} = g_{\theta^\prime} (\tilde{\mathbf{y}}) = \Phi ( \mathbf{W}^{(H) \prime}[...[\sigma ( \mathbf{W}^{(B+2) \prime}  [\sigma (\mathbf{W}^{(B+1) \prime} \tilde{\mathbf{y}} + \mathbf{b}^{(B+1) \prime})] + \mathbf{b}^{(B+2) \prime})]...] + \mathbf{b}^{(H) \prime})
\end{align*}
\normalsize
where $g \mathrel{\dot\sim} \text{GP}$ and $\mathbf{z}$ represents a fully observed vector containing predictions of $\tilde{\mathbf{x}}_\text{obs}$ and $\tilde{\mathbf{x}}_\text{mis}$. 

## Activation functions

MIDAS use the exponential linear unit as its default activation function

* Facilitates efficient training in deep neural networks

The final-layer activation function is:

* Identity if $\mathbf{x}$ is continuous
* Logistic if $\mathbf{x}$ is binary
* Softmax if $\mathbf{x}$ is categorical

MIDAS measures loss with respect to deliberately corrupted values only ($\tilde{\mathbf{x}}_\text{obs}$) by multiplying by the corruption indicator $\mathbf{r}$

Loss calculations:
\small
\begin{align*}
L(\mathbf{x},\mathbf{z}, \mathbf{r}) =
\begin{cases}
[\frac{1}{J} \sum_{j=1}^J\mathbf{r}_j(\mathbf{x}_j-\mathbf{z}_j)^2]^{\frac{1}{2}} & \text{if $\mathbf{x}$ is continuous} \\
-\frac{1}{J} \sum_{j=1}^J \mathbf{r}_j[\mathbf{x}_j \log \mathbf{z}_j + (1-\mathbf{x}_j) \log (1-\mathbf{z}_j)] & \text{if $\mathbf{x}$ is categorical.}
\end{cases}
\label{eq:loss}
\end{align*}


## MIDAS algorithm

![Algorithm Steps for training a MIDAS network](images/MIDAS_steps.pdf)

## Flexibily impute time trends

Existing strategies also struggle with non-exchangeable data

* E.g. panel data
* Most assume linear independence between observations
* Or require additional time polynomial terms
* MIDAS is complex enough to implicitly detect these trends

\vspace{-1.5em}
![WDI Imputation](images/wdi_large.pdf)

## Relevance of neural networks to the social sciences

MIDAS is one example of deep learning in a social science framework

* Used to solve a particular prediction problem: missing data
* Leverage predictive power and combine with a known inference method

One particularly exciting avenue for neural net. applications is in "creative" tasks:

* Constructing more realistic vignettes
* Synthesizing data

And there are applied subjects of interest too:

* How does the rise of neural networks affect political behavior?
    * E.g. Does political ad targeting work? 
    * See here: https://muhark.github.io/static/docs/harukawa-2021-microtargeting.pdf

